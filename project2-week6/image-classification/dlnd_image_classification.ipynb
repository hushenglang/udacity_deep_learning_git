{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3153b23c50>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return x/255\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    one_hot_encoder_lables = np.zeros((len(x),10))\n",
    "    for idx, val in enumerate(x):\n",
    "        one_hot_encoder_lables[idx][val] = 1\n",
    "    return one_hot_encoder_lables\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape = [None, image_shape[0],image_shape[1], image_shape[2]], name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape = [None, n_classes], name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    in_channels = x_tensor.get_shape()[3].value\n",
    "    F_W = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], in_channels, conv_num_outputs], \n",
    "                                          mean=0.0, stddev=0.1, seed=42))\n",
    "    F_B = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, F_W, [1,conv_strides[0],conv_strides[1],1], 'SAME')+F_B\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    conv_layer_maxpool = tf.nn.max_pool(conv_layer, [1,pool_ksize[0],pool_ksize[1],1], [1,pool_strides[0],pool_strides[1],1], 'SAME')\n",
    "    \n",
    "    return conv_layer_maxpool \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    feature_count = x_tensor.get_shape()[1].value\n",
    "    weight = tf.Variable(tf.truncated_normal([feature_count, num_outputs], \n",
    "                                          mean=0.0, stddev=0.1, seed=42))\n",
    "    bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    logits = tf.matmul(x_tensor , weight) + bias\n",
    "    logits = tf.nn.relu(logits)\n",
    "    return logits\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    feature_count = x_tensor.get_shape()[1].value\n",
    "    weight = tf.Variable(tf.truncated_normal([feature_count, num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    logits = tf.matmul(x_tensor , weight) + bias\n",
    "    return logits\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    \n",
    "    #convolution layers#\n",
    "    conv_ksize = (2,2)\n",
    "    conv_strides = (1,1)\n",
    "    pool_ksize = (2,2)\n",
    "    pool_strides = (1,1)\n",
    "    \n",
    "    # conv layer-1\n",
    "    cl1_conv_num_outputs = 16\n",
    "    conv_layer1 = conv2d_maxpool(x, cl1_conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    # conv layer-2\n",
    "    cl2_conv_num_outputs = 24\n",
    "    conv_layer2 = conv2d_maxpool(conv_layer1, cl2_conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    # conv layer-3\n",
    "    #cl3_conv_num_outputs = 30\n",
    "    #conv_layer3 = conv2d_maxpool(conv_layer2, cl3_conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "        \n",
    "        \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    conv_layer = flatten(conv_layer2)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    # fully layer 1\n",
    "    fl1_num_outputs = 1024\n",
    "    ful_conn_layer1 = fully_conn(conv_layer, fl1_num_outputs)\n",
    "    \n",
    "    # fully layer 2\n",
    "    #fl2_num_outputs = 256\n",
    "    #ful_conn_layer2 = fully_conn(ful_conn_layer1, fl2_num_outputs)\n",
    "    \n",
    "    # fully layer 3\n",
    "    #fl3_num_outputs = 20\n",
    "    #ful_conn_layer3 = fully_conn(ful_conn_layer2, fl3_num_outputs)\n",
    "    \n",
    "    ful_conn_layer = tf.nn.dropout(ful_conn_layer1, keep_prob)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    num_class = 10\n",
    "    out_layer = output(ful_conn_layer, num_class)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={\n",
    "        x:feature_batch,\n",
    "        y:label_batch,\n",
    "        keep_prob:keep_probability\n",
    "    })\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={\n",
    "        x:feature_batch,\n",
    "        y:label_batch,\n",
    "        keep_prob:1.0\n",
    "    })\n",
    "    \n",
    "    train_accuracy = session.run(accuracy, feed_dict={\n",
    "        x:feature_batch,\n",
    "        y:label_batch,\n",
    "        keep_prob:1.0\n",
    "    })\n",
    "\n",
    "    validate_accuracy = session.run(accuracy, feed_dict={\n",
    "        x:valid_features,\n",
    "        y:valid_labels,\n",
    "        keep_prob:1.0\n",
    "    })\n",
    "    \n",
    "    print(\"loss: {0:.6f}  validate_accuracy: {1:.6f}  train_accuracy: {2:.6f}\".format(loss, validate_accuracy, train_accuracy))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 1280\n",
    "keep_probability = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss: 18.530678  validate_accuracy: 0.148400  train_accuracy: 0.050000\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss: 4.765692  validate_accuracy: 0.147200  train_accuracy: 0.175000\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss: 2.026091  validate_accuracy: 0.211000  train_accuracy: 0.350000\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss: 1.500925  validate_accuracy: 0.236800  train_accuracy: 0.575000\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss: 1.203588  validate_accuracy: 0.299600  train_accuracy: 0.650000\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss: 0.891298  validate_accuracy: 0.351400  train_accuracy: 0.750000\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss: 0.583564  validate_accuracy: 0.390200  train_accuracy: 0.875000\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss: 0.376342  validate_accuracy: 0.420200  train_accuracy: 0.950000\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss: 0.232904  validate_accuracy: 0.435400  train_accuracy: 1.000000\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss: 0.152705  validate_accuracy: 0.452200  train_accuracy: 1.000000\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss: 0.091789  validate_accuracy: 0.461800  train_accuracy: 1.000000\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss: 0.065512  validate_accuracy: 0.459400  train_accuracy: 1.000000\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss: 0.042119  validate_accuracy: 0.475800  train_accuracy: 1.000000\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss: 0.034634  validate_accuracy: 0.478400  train_accuracy: 1.000000\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss: 0.023477  validate_accuracy: 0.483000  train_accuracy: 1.000000\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss: 0.022240  validate_accuracy: 0.481800  train_accuracy: 1.000000\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss: 0.016913  validate_accuracy: 0.482600  train_accuracy: 1.000000\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss: 0.015855  validate_accuracy: 0.480800  train_accuracy: 1.000000\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss: 0.015224  validate_accuracy: 0.454400  train_accuracy: 1.000000\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss: 0.013664  validate_accuracy: 0.471600  train_accuracy: 1.000000\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss: 0.010237  validate_accuracy: 0.485600  train_accuracy: 1.000000\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss: 0.008648  validate_accuracy: 0.464600  train_accuracy: 1.000000\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss: 0.008355  validate_accuracy: 0.495800  train_accuracy: 1.000000\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss: 0.006530  validate_accuracy: 0.492200  train_accuracy: 1.000000\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss: 0.006690  validate_accuracy: 0.503800  train_accuracy: 1.000000\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss: 0.006006  validate_accuracy: 0.506200  train_accuracy: 1.000000\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss: 0.003188  validate_accuracy: 0.517400  train_accuracy: 1.000000\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss: 0.003038  validate_accuracy: 0.512000  train_accuracy: 1.000000\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss: 0.002338  validate_accuracy: 0.506800  train_accuracy: 1.000000\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss: 0.004844  validate_accuracy: 0.505600  train_accuracy: 1.000000\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss: 0.003208  validate_accuracy: 0.510400  train_accuracy: 1.000000\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss: 0.003910  validate_accuracy: 0.499200  train_accuracy: 1.000000\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss: 0.001871  validate_accuracy: 0.506800  train_accuracy: 1.000000\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss: 0.001663  validate_accuracy: 0.506400  train_accuracy: 1.000000\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss: 0.001139  validate_accuracy: 0.509800  train_accuracy: 1.000000\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss: 0.001471  validate_accuracy: 0.505000  train_accuracy: 1.000000\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss: 0.001255  validate_accuracy: 0.499800  train_accuracy: 1.000000\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss: 0.002180  validate_accuracy: 0.492600  train_accuracy: 1.000000\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss: 0.002335  validate_accuracy: 0.496200  train_accuracy: 1.000000\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss: 0.001551  validate_accuracy: 0.503000  train_accuracy: 1.000000\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss: 0.000897  validate_accuracy: 0.518800  train_accuracy: 1.000000\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss: 0.000790  validate_accuracy: 0.521800  train_accuracy: 1.000000\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss: 0.000457  validate_accuracy: 0.520400  train_accuracy: 1.000000\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss: 0.000433  validate_accuracy: 0.521600  train_accuracy: 1.000000\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss: 0.000366  validate_accuracy: 0.525400  train_accuracy: 1.000000\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss: 0.000356  validate_accuracy: 0.519600  train_accuracy: 1.000000\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss: 0.000277  validate_accuracy: 0.527200  train_accuracy: 1.000000\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss: 0.000275  validate_accuracy: 0.525000  train_accuracy: 1.000000\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss: 0.000321  validate_accuracy: 0.524200  train_accuracy: 1.000000\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss: 0.000275  validate_accuracy: 0.519000  train_accuracy: 1.000000\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss: 0.000270  validate_accuracy: 0.526800  train_accuracy: 1.000000\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss: 0.000219  validate_accuracy: 0.521600  train_accuracy: 1.000000\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss: 0.000227  validate_accuracy: 0.529400  train_accuracy: 1.000000\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss: 0.000236  validate_accuracy: 0.528200  train_accuracy: 1.000000\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss: 0.000205  validate_accuracy: 0.524800  train_accuracy: 1.000000\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss: 0.000188  validate_accuracy: 0.528800  train_accuracy: 1.000000\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss: 0.000230  validate_accuracy: 0.522800  train_accuracy: 1.000000\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss: 0.000182  validate_accuracy: 0.524600  train_accuracy: 1.000000\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss: 0.000171  validate_accuracy: 0.527800  train_accuracy: 1.000000\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss: 0.000171  validate_accuracy: 0.533400  train_accuracy: 1.000000\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss: 0.000153  validate_accuracy: 0.524800  train_accuracy: 1.000000\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss: 0.000147  validate_accuracy: 0.533600  train_accuracy: 1.000000\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss: 0.000157  validate_accuracy: 0.525400  train_accuracy: 1.000000\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss: 0.000131  validate_accuracy: 0.531400  train_accuracy: 1.000000\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss: 0.000136  validate_accuracy: 0.524000  train_accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss: 0.000111  validate_accuracy: 0.528600  train_accuracy: 1.000000\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss: 0.000108  validate_accuracy: 0.525200  train_accuracy: 1.000000\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss: 0.000101  validate_accuracy: 0.529200  train_accuracy: 1.000000\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss: 0.000103  validate_accuracy: 0.532800  train_accuracy: 1.000000\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss: 0.000125  validate_accuracy: 0.532000  train_accuracy: 1.000000\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss: 0.000103  validate_accuracy: 0.530000  train_accuracy: 1.000000\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss: 0.000071  validate_accuracy: 0.533800  train_accuracy: 1.000000\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss: 0.000092  validate_accuracy: 0.529800  train_accuracy: 1.000000\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss: 0.000090  validate_accuracy: 0.529600  train_accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss: 0.000086  validate_accuracy: 0.530800  train_accuracy: 1.000000\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss: 0.000079  validate_accuracy: 0.530200  train_accuracy: 1.000000\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss: 0.000093  validate_accuracy: 0.534400  train_accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss: 0.000077  validate_accuracy: 0.528800  train_accuracy: 1.000000\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss: 0.000068  validate_accuracy: 0.530600  train_accuracy: 1.000000\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss: 0.000083  validate_accuracy: 0.530400  train_accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss: 0.000061  validate_accuracy: 0.535000  train_accuracy: 1.000000\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss: 0.000060  validate_accuracy: 0.528800  train_accuracy: 1.000000\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss: 0.000064  validate_accuracy: 0.534400  train_accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss: 0.000048  validate_accuracy: 0.531600  train_accuracy: 1.000000\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss: 0.000051  validate_accuracy: 0.534200  train_accuracy: 1.000000\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss: 0.000042  validate_accuracy: 0.525200  train_accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss: 0.000042  validate_accuracy: 0.535200  train_accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss: 0.000045  validate_accuracy: 0.527000  train_accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss: 0.000050  validate_accuracy: 0.527800  train_accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss: 0.000034  validate_accuracy: 0.531000  train_accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss: 0.000045  validate_accuracy: 0.535200  train_accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss: 0.000043  validate_accuracy: 0.535400  train_accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss: 0.000033  validate_accuracy: 0.532600  train_accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss: 0.000034  validate_accuracy: 0.533400  train_accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss: 0.000032  validate_accuracy: 0.535400  train_accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss: 0.000029  validate_accuracy: 0.537200  train_accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss: 0.000035  validate_accuracy: 0.524600  train_accuracy: 1.000000\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss: 0.000036  validate_accuracy: 0.534000  train_accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss: 0.000030  validate_accuracy: 0.531600  train_accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss: 0.000042  validate_accuracy: 0.534800  train_accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss: 22.641281  validate_accuracy: 0.135200  train_accuracy: 0.125000\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss: 5.120378  validate_accuracy: 0.175800  train_accuracy: 0.275000\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss: 2.444420  validate_accuracy: 0.190200  train_accuracy: 0.075000\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss: 1.972931  validate_accuracy: 0.218800  train_accuracy: 0.375000\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss: 1.898763  validate_accuracy: 0.309400  train_accuracy: 0.325000\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss: 1.625326  validate_accuracy: 0.350400  train_accuracy: 0.475000\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss: 1.473900  validate_accuracy: 0.382200  train_accuracy: 0.525000\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss: 1.518577  validate_accuracy: 0.401400  train_accuracy: 0.475000\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss: 1.171403  validate_accuracy: 0.422000  train_accuracy: 0.650000\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss: 1.106044  validate_accuracy: 0.428600  train_accuracy: 0.750000\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss: 0.984394  validate_accuracy: 0.439200  train_accuracy: 0.675000\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss: 0.719986  validate_accuracy: 0.451400  train_accuracy: 0.875000\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss: 0.658319  validate_accuracy: 0.473000  train_accuracy: 0.825000\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss: 0.570953  validate_accuracy: 0.478000  train_accuracy: 0.800000\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss: 0.483491  validate_accuracy: 0.497400  train_accuracy: 0.975000\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss: 0.542820  validate_accuracy: 0.504200  train_accuracy: 0.850000\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss: 0.368403  validate_accuracy: 0.494600  train_accuracy: 0.975000\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss: 0.276494  validate_accuracy: 0.512200  train_accuracy: 0.975000\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss: 0.265213  validate_accuracy: 0.517600  train_accuracy: 1.000000\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss: 0.220013  validate_accuracy: 0.534000  train_accuracy: 1.000000\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss: 0.316989  validate_accuracy: 0.522800  train_accuracy: 0.925000\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss: 0.147544  validate_accuracy: 0.534000  train_accuracy: 1.000000\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss: 0.136832  validate_accuracy: 0.540200  train_accuracy: 1.000000\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss: 0.135666  validate_accuracy: 0.541000  train_accuracy: 1.000000\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss: 0.108605  validate_accuracy: 0.552200  train_accuracy: 1.000000\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss: 0.165429  validate_accuracy: 0.548800  train_accuracy: 1.000000\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss: 0.073993  validate_accuracy: 0.552400  train_accuracy: 1.000000\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss: 0.076809  validate_accuracy: 0.555600  train_accuracy: 1.000000\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss: 0.072567  validate_accuracy: 0.561600  train_accuracy: 1.000000\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss: 0.069088  validate_accuracy: 0.561800  train_accuracy: 1.000000\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss: 0.081717  validate_accuracy: 0.551400  train_accuracy: 1.000000\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss: 0.045498  validate_accuracy: 0.577200  train_accuracy: 1.000000\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss: 0.044915  validate_accuracy: 0.568800  train_accuracy: 1.000000\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss: 0.048341  validate_accuracy: 0.573600  train_accuracy: 1.000000\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss: 0.046900  validate_accuracy: 0.580400  train_accuracy: 1.000000\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss: 0.046391  validate_accuracy: 0.575200  train_accuracy: 1.000000\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss: 0.029531  validate_accuracy: 0.585800  train_accuracy: 1.000000\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss: 0.032617  validate_accuracy: 0.582400  train_accuracy: 1.000000\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss: 0.034916  validate_accuracy: 0.582400  train_accuracy: 1.000000\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss: 0.034112  validate_accuracy: 0.586200  train_accuracy: 1.000000\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss: 0.034014  validate_accuracy: 0.580400  train_accuracy: 1.000000\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss: 0.021003  validate_accuracy: 0.581400  train_accuracy: 1.000000\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss: 0.025125  validate_accuracy: 0.587000  train_accuracy: 1.000000\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss: 0.032532  validate_accuracy: 0.582800  train_accuracy: 1.000000\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss: 0.027398  validate_accuracy: 0.587000  train_accuracy: 1.000000\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss: 0.025979  validate_accuracy: 0.578000  train_accuracy: 1.000000\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss: 0.025220  validate_accuracy: 0.565800  train_accuracy: 1.000000\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss: 0.021798  validate_accuracy: 0.570800  train_accuracy: 1.000000\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss: 0.018299  validate_accuracy: 0.567400  train_accuracy: 1.000000\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss: 0.030236  validate_accuracy: 0.579600  train_accuracy: 1.000000\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss: 0.030859  validate_accuracy: 0.584800  train_accuracy: 1.000000\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss: 0.029426  validate_accuracy: 0.556400  train_accuracy: 1.000000\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss: 0.014337  validate_accuracy: 0.562000  train_accuracy: 1.000000\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss: 0.019748  validate_accuracy: 0.579200  train_accuracy: 1.000000\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss: 0.031295  validate_accuracy: 0.584600  train_accuracy: 1.000000\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss: 0.020171  validate_accuracy: 0.583000  train_accuracy: 1.000000\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss: 0.020698  validate_accuracy: 0.591800  train_accuracy: 1.000000\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss: 0.011831  validate_accuracy: 0.587400  train_accuracy: 1.000000\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss: 0.015005  validate_accuracy: 0.585600  train_accuracy: 1.000000\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss: 0.015085  validate_accuracy: 0.582400  train_accuracy: 1.000000\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss: 0.013714  validate_accuracy: 0.584200  train_accuracy: 1.000000\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss: 0.013326  validate_accuracy: 0.582800  train_accuracy: 1.000000\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss: 0.009703  validate_accuracy: 0.585600  train_accuracy: 1.000000\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss: 0.009571  validate_accuracy: 0.583400  train_accuracy: 1.000000\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss: 0.014849  validate_accuracy: 0.586400  train_accuracy: 1.000000\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss: 0.016823  validate_accuracy: 0.601200  train_accuracy: 1.000000\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss: 0.008854  validate_accuracy: 0.583200  train_accuracy: 1.000000\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss: 0.007834  validate_accuracy: 0.599600  train_accuracy: 1.000000\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss: 0.009331  validate_accuracy: 0.591000  train_accuracy: 1.000000\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss: 0.009358  validate_accuracy: 0.607600  train_accuracy: 1.000000\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss: 0.014863  validate_accuracy: 0.606200  train_accuracy: 1.000000\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss: 0.004322  validate_accuracy: 0.603000  train_accuracy: 1.000000\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss: 0.007123  validate_accuracy: 0.599200  train_accuracy: 1.000000\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss: 0.010404  validate_accuracy: 0.592800  train_accuracy: 1.000000\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss: 0.009960  validate_accuracy: 0.593400  train_accuracy: 1.000000\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss: 0.017858  validate_accuracy: 0.583400  train_accuracy: 1.000000\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss: 0.004620  validate_accuracy: 0.596200  train_accuracy: 1.000000\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss: 0.006817  validate_accuracy: 0.603200  train_accuracy: 1.000000\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss: 0.004878  validate_accuracy: 0.593200  train_accuracy: 1.000000\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss: 0.005402  validate_accuracy: 0.609000  train_accuracy: 1.000000\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss: 0.008114  validate_accuracy: 0.598400  train_accuracy: 1.000000\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss: 0.002635  validate_accuracy: 0.596800  train_accuracy: 1.000000\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss: 0.004240  validate_accuracy: 0.603000  train_accuracy: 1.000000\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss: 0.004172  validate_accuracy: 0.591800  train_accuracy: 1.000000\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss: 0.005177  validate_accuracy: 0.596200  train_accuracy: 1.000000\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss: 0.007353  validate_accuracy: 0.595400  train_accuracy: 1.000000\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss: 0.003165  validate_accuracy: 0.597200  train_accuracy: 1.000000\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss: 0.002416  validate_accuracy: 0.596600  train_accuracy: 1.000000\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss: 0.004283  validate_accuracy: 0.605000  train_accuracy: 1.000000\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss: 0.004278  validate_accuracy: 0.599800  train_accuracy: 1.000000\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss: 0.004414  validate_accuracy: 0.610400  train_accuracy: 1.000000\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss: 0.002374  validate_accuracy: 0.609400  train_accuracy: 1.000000\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss: 0.003648  validate_accuracy: 0.606400  train_accuracy: 1.000000\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss: 0.005260  validate_accuracy: 0.598400  train_accuracy: 1.000000\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss: 0.003487  validate_accuracy: 0.599800  train_accuracy: 1.000000\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss: 0.002859  validate_accuracy: 0.615600  train_accuracy: 1.000000\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss: 0.001928  validate_accuracy: 0.615800  train_accuracy: 1.000000\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss: 0.003343  validate_accuracy: 0.615200  train_accuracy: 1.000000\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss: 0.003922  validate_accuracy: 0.608200  train_accuracy: 1.000000\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss: 0.003780  validate_accuracy: 0.600600  train_accuracy: 1.000000\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss: 0.002113  validate_accuracy: 0.617800  train_accuracy: 1.000000\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss: 0.002203  validate_accuracy: 0.609800  train_accuracy: 1.000000\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss: 0.001946  validate_accuracy: 0.609600  train_accuracy: 1.000000\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss: 0.002356  validate_accuracy: 0.610800  train_accuracy: 1.000000\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss: 0.002041  validate_accuracy: 0.618400  train_accuracy: 1.000000\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss: 0.002449  validate_accuracy: 0.612600  train_accuracy: 1.000000\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss: 0.001321  validate_accuracy: 0.616200  train_accuracy: 1.000000\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss: 0.002896  validate_accuracy: 0.604600  train_accuracy: 1.000000\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss: 0.002610  validate_accuracy: 0.604600  train_accuracy: 1.000000\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss: 0.001667  validate_accuracy: 0.607600  train_accuracy: 1.000000\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss: 0.002324  validate_accuracy: 0.610000  train_accuracy: 1.000000\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss: 0.002046  validate_accuracy: 0.597800  train_accuracy: 1.000000\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss: 0.004412  validate_accuracy: 0.562800  train_accuracy: 1.000000\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss: 0.004460  validate_accuracy: 0.579600  train_accuracy: 1.000000\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss: 0.001889  validate_accuracy: 0.608400  train_accuracy: 1.000000\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss: 0.002758  validate_accuracy: 0.619400  train_accuracy: 1.000000\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss: 0.001449  validate_accuracy: 0.623600  train_accuracy: 1.000000\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss: 0.001116  validate_accuracy: 0.613600  train_accuracy: 1.000000\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss: 0.002626  validate_accuracy: 0.593600  train_accuracy: 1.000000\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss: 0.000946  validate_accuracy: 0.622800  train_accuracy: 1.000000\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss: 0.002072  validate_accuracy: 0.616800  train_accuracy: 1.000000\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss: 0.001127  validate_accuracy: 0.614000  train_accuracy: 1.000000\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss: 0.001212  validate_accuracy: 0.613000  train_accuracy: 1.000000\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss: 0.001809  validate_accuracy: 0.617600  train_accuracy: 1.000000\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss: 0.001081  validate_accuracy: 0.618200  train_accuracy: 1.000000\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss: 0.001609  validate_accuracy: 0.613200  train_accuracy: 1.000000\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss: 0.000906  validate_accuracy: 0.615400  train_accuracy: 1.000000\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss: 0.000866  validate_accuracy: 0.610800  train_accuracy: 1.000000\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss: 0.000944  validate_accuracy: 0.618200  train_accuracy: 1.000000\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss: 0.001106  validate_accuracy: 0.619000  train_accuracy: 1.000000\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss: 0.001289  validate_accuracy: 0.622600  train_accuracy: 1.000000\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss: 0.000934  validate_accuracy: 0.605400  train_accuracy: 1.000000\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss: 0.000928  validate_accuracy: 0.611600  train_accuracy: 1.000000\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss: 0.001194  validate_accuracy: 0.616400  train_accuracy: 1.000000\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss: 0.000759  validate_accuracy: 0.610400  train_accuracy: 1.000000\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss: 0.000641  validate_accuracy: 0.630200  train_accuracy: 1.000000\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss: 0.000420  validate_accuracy: 0.610600  train_accuracy: 1.000000\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss: 0.000559  validate_accuracy: 0.628200  train_accuracy: 1.000000\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss: 0.000835  validate_accuracy: 0.622200  train_accuracy: 1.000000\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss: 0.000649  validate_accuracy: 0.610000  train_accuracy: 1.000000\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss: 0.001021  validate_accuracy: 0.621600  train_accuracy: 1.000000\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss: 0.000918  validate_accuracy: 0.603600  train_accuracy: 1.000000\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss: 0.000228  validate_accuracy: 0.633000  train_accuracy: 1.000000\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss: 0.000670  validate_accuracy: 0.615400  train_accuracy: 1.000000\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss: 0.000457  validate_accuracy: 0.609600  train_accuracy: 1.000000\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss: 0.000902  validate_accuracy: 0.623000  train_accuracy: 1.000000\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss: 0.000694  validate_accuracy: 0.614800  train_accuracy: 1.000000\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss: 0.000282  validate_accuracy: 0.616600  train_accuracy: 1.000000\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss: 0.000686  validate_accuracy: 0.626400  train_accuracy: 1.000000\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss: 0.000449  validate_accuracy: 0.634800  train_accuracy: 1.000000\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss: 0.000533  validate_accuracy: 0.631800  train_accuracy: 1.000000\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss: 0.000306  validate_accuracy: 0.627000  train_accuracy: 1.000000\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss: 0.000245  validate_accuracy: 0.624200  train_accuracy: 1.000000\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss: 0.000611  validate_accuracy: 0.613600  train_accuracy: 1.000000\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss: 0.000684  validate_accuracy: 0.629800  train_accuracy: 1.000000\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss: 0.000410  validate_accuracy: 0.628800  train_accuracy: 1.000000\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss: 0.000268  validate_accuracy: 0.627800  train_accuracy: 1.000000\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss: 0.000320  validate_accuracy: 0.624200  train_accuracy: 1.000000\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss: 0.000538  validate_accuracy: 0.605000  train_accuracy: 1.000000\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss: 0.002080  validate_accuracy: 0.614800  train_accuracy: 1.000000\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss: 0.000313  validate_accuracy: 0.630000  train_accuracy: 1.000000\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss: 0.000452  validate_accuracy: 0.630000  train_accuracy: 1.000000\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss: 0.000164  validate_accuracy: 0.630600  train_accuracy: 1.000000\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss: 0.000265  validate_accuracy: 0.620800  train_accuracy: 1.000000\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss: 0.000371  validate_accuracy: 0.610600  train_accuracy: 1.000000\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss: 0.000336  validate_accuracy: 0.623000  train_accuracy: 1.000000\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss: 0.000100  validate_accuracy: 0.631600  train_accuracy: 1.000000\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss: 0.000180  validate_accuracy: 0.636800  train_accuracy: 1.000000\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss: 0.000223  validate_accuracy: 0.628000  train_accuracy: 1.000000\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss: 0.000266  validate_accuracy: 0.623400  train_accuracy: 1.000000\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss: 0.000417  validate_accuracy: 0.619800  train_accuracy: 1.000000\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss: 0.000099  validate_accuracy: 0.626400  train_accuracy: 1.000000\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss: 0.000184  validate_accuracy: 0.636600  train_accuracy: 1.000000\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss: 0.000238  validate_accuracy: 0.630000  train_accuracy: 1.000000\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss: 0.000306  validate_accuracy: 0.629600  train_accuracy: 1.000000\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss: 0.000356  validate_accuracy: 0.623200  train_accuracy: 1.000000\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss: 0.000162  validate_accuracy: 0.616800  train_accuracy: 1.000000\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss: 0.000198  validate_accuracy: 0.617400  train_accuracy: 1.000000\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss: 0.000264  validate_accuracy: 0.624200  train_accuracy: 1.000000\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss: 0.000262  validate_accuracy: 0.629200  train_accuracy: 1.000000\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss: 0.000408  validate_accuracy: 0.627200  train_accuracy: 1.000000\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss: 0.000094  validate_accuracy: 0.634200  train_accuracy: 1.000000\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss: 0.000170  validate_accuracy: 0.621400  train_accuracy: 1.000000\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss: 0.000335  validate_accuracy: 0.623000  train_accuracy: 1.000000\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss: 0.000370  validate_accuracy: 0.627000  train_accuracy: 1.000000\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss: 0.000303  validate_accuracy: 0.635400  train_accuracy: 1.000000\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss: 0.000099  validate_accuracy: 0.636800  train_accuracy: 1.000000\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss: 0.000100  validate_accuracy: 0.625800  train_accuracy: 1.000000\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss: 0.000334  validate_accuracy: 0.630400  train_accuracy: 1.000000\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss: 0.000162  validate_accuracy: 0.632400  train_accuracy: 1.000000\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss: 0.000307  validate_accuracy: 0.639600  train_accuracy: 1.000000\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss: 0.000100  validate_accuracy: 0.630200  train_accuracy: 1.000000\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss: 0.000100  validate_accuracy: 0.635800  train_accuracy: 1.000000\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss: 0.000186  validate_accuracy: 0.633600  train_accuracy: 1.000000\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss: 0.000139  validate_accuracy: 0.621800  train_accuracy: 1.000000\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss: 0.000422  validate_accuracy: 0.633200  train_accuracy: 1.000000\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss: 0.000072  validate_accuracy: 0.637600  train_accuracy: 1.000000\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss: 0.000095  validate_accuracy: 0.641600  train_accuracy: 1.000000\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss: 0.000205  validate_accuracy: 0.636600  train_accuracy: 1.000000\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss: 0.000155  validate_accuracy: 0.616400  train_accuracy: 1.000000\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss: 0.000525  validate_accuracy: 0.614000  train_accuracy: 1.000000\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss: 0.000220  validate_accuracy: 0.630800  train_accuracy: 1.000000\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss: 0.000089  validate_accuracy: 0.627200  train_accuracy: 1.000000\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss: 0.000190  validate_accuracy: 0.640200  train_accuracy: 1.000000\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss: 0.000104  validate_accuracy: 0.628400  train_accuracy: 1.000000\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss: 0.000338  validate_accuracy: 0.630800  train_accuracy: 1.000000\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss: 0.000169  validate_accuracy: 0.632000  train_accuracy: 1.000000\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss: 0.000114  validate_accuracy: 0.619600  train_accuracy: 1.000000\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss: 0.000257  validate_accuracy: 0.632000  train_accuracy: 1.000000\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss: 0.000124  validate_accuracy: 0.634200  train_accuracy: 1.000000\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss: 0.000193  validate_accuracy: 0.634600  train_accuracy: 1.000000\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss: 0.000174  validate_accuracy: 0.637600  train_accuracy: 1.000000\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss: 0.000089  validate_accuracy: 0.622800  train_accuracy: 1.000000\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss: 0.000186  validate_accuracy: 0.632400  train_accuracy: 1.000000\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss: 0.000067  validate_accuracy: 0.635200  train_accuracy: 1.000000\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss: 0.000213  validate_accuracy: 0.631600  train_accuracy: 1.000000\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss: 0.000033  validate_accuracy: 0.642400  train_accuracy: 1.000000\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss: 0.000037  validate_accuracy: 0.628600  train_accuracy: 1.000000\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss: 0.000146  validate_accuracy: 0.636200  train_accuracy: 1.000000\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss: 0.000070  validate_accuracy: 0.628400  train_accuracy: 1.000000\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss: 0.000246  validate_accuracy: 0.642200  train_accuracy: 1.000000\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss: 0.000030  validate_accuracy: 0.639800  train_accuracy: 1.000000\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss: 0.000033  validate_accuracy: 0.634200  train_accuracy: 1.000000\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss: 0.000083  validate_accuracy: 0.640000  train_accuracy: 1.000000\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss: 0.000082  validate_accuracy: 0.633200  train_accuracy: 1.000000\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss: 0.000188  validate_accuracy: 0.633200  train_accuracy: 1.000000\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss: 0.000036  validate_accuracy: 0.639600  train_accuracy: 1.000000\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss: 0.000035  validate_accuracy: 0.636800  train_accuracy: 1.000000\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss: 0.000130  validate_accuracy: 0.627400  train_accuracy: 1.000000\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss: 0.000086  validate_accuracy: 0.633000  train_accuracy: 1.000000\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss: 0.000154  validate_accuracy: 0.628200  train_accuracy: 1.000000\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss: 0.000020  validate_accuracy: 0.636600  train_accuracy: 1.000000\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss: 0.000058  validate_accuracy: 0.629200  train_accuracy: 1.000000\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss: 0.000076  validate_accuracy: 0.634600  train_accuracy: 1.000000\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss: 0.000098  validate_accuracy: 0.630200  train_accuracy: 1.000000\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss: 0.000138  validate_accuracy: 0.629800  train_accuracy: 1.000000\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss: 0.000029  validate_accuracy: 0.635800  train_accuracy: 1.000000\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss: 0.000041  validate_accuracy: 0.636600  train_accuracy: 1.000000\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss: 0.000063  validate_accuracy: 0.642400  train_accuracy: 1.000000\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss: 0.000056  validate_accuracy: 0.632400  train_accuracy: 1.000000\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss: 0.000097  validate_accuracy: 0.640400  train_accuracy: 1.000000\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss: 0.000021  validate_accuracy: 0.642400  train_accuracy: 1.000000\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss: 0.000047  validate_accuracy: 0.646600  train_accuracy: 1.000000\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss: 0.000059  validate_accuracy: 0.638600  train_accuracy: 1.000000\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss: 0.000062  validate_accuracy: 0.634000  train_accuracy: 1.000000\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss: 0.000084  validate_accuracy: 0.631000  train_accuracy: 1.000000\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss: 0.000033  validate_accuracy: 0.644000  train_accuracy: 1.000000\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss: 0.000054  validate_accuracy: 0.634600  train_accuracy: 1.000000\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss: 0.000081  validate_accuracy: 0.632400  train_accuracy: 1.000000\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss: 0.000088  validate_accuracy: 0.622800  train_accuracy: 1.000000\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss: 0.000115  validate_accuracy: 0.623200  train_accuracy: 1.000000\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss: 0.000049  validate_accuracy: 0.631600  train_accuracy: 1.000000\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss: 0.000045  validate_accuracy: 0.630800  train_accuracy: 1.000000\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss: 0.000048  validate_accuracy: 0.633600  train_accuracy: 1.000000\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss: 0.000067  validate_accuracy: 0.634600  train_accuracy: 1.000000\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss: 0.000088  validate_accuracy: 0.630400  train_accuracy: 1.000000\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss: 0.000028  validate_accuracy: 0.641000  train_accuracy: 1.000000\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss: 0.000017  validate_accuracy: 0.628400  train_accuracy: 1.000000\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss: 0.000053  validate_accuracy: 0.636000  train_accuracy: 1.000000\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss: 0.000039  validate_accuracy: 0.628400  train_accuracy: 1.000000\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss: 0.000077  validate_accuracy: 0.637200  train_accuracy: 1.000000\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss: 0.000038  validate_accuracy: 0.639400  train_accuracy: 1.000000\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss: 0.000026  validate_accuracy: 0.640600  train_accuracy: 1.000000\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss: 0.000063  validate_accuracy: 0.629600  train_accuracy: 1.000000\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss: 0.000060  validate_accuracy: 0.624600  train_accuracy: 1.000000\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss: 0.000141  validate_accuracy: 0.631800  train_accuracy: 1.000000\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss: 0.000027  validate_accuracy: 0.635800  train_accuracy: 1.000000\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss: 0.000017  validate_accuracy: 0.641200  train_accuracy: 1.000000\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss: 0.000078  validate_accuracy: 0.627600  train_accuracy: 1.000000\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss: 0.000049  validate_accuracy: 0.626400  train_accuracy: 1.000000\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss: 0.000069  validate_accuracy: 0.631000  train_accuracy: 1.000000\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss: 0.000043  validate_accuracy: 0.639600  train_accuracy: 1.000000\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss: 0.000028  validate_accuracy: 0.633000  train_accuracy: 1.000000\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss: 0.000053  validate_accuracy: 0.633400  train_accuracy: 1.000000\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss: 0.000064  validate_accuracy: 0.632800  train_accuracy: 1.000000\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss: 0.000061  validate_accuracy: 0.639200  train_accuracy: 1.000000\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss: 0.000028  validate_accuracy: 0.640400  train_accuracy: 1.000000\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss: 0.000027  validate_accuracy: 0.644200  train_accuracy: 1.000000\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss: 0.000025  validate_accuracy: 0.634800  train_accuracy: 1.000000\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss: 0.000036  validate_accuracy: 0.634800  train_accuracy: 1.000000\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss: 0.000039  validate_accuracy: 0.636800  train_accuracy: 1.000000\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss: 0.000037  validate_accuracy: 0.637800  train_accuracy: 1.000000\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss: 0.000028  validate_accuracy: 0.640200  train_accuracy: 1.000000\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss: 0.000037  validate_accuracy: 0.633800  train_accuracy: 1.000000\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss: 0.000023  validate_accuracy: 0.634800  train_accuracy: 1.000000\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss: 0.000051  validate_accuracy: 0.645000  train_accuracy: 1.000000\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss: 0.000023  validate_accuracy: 0.644600  train_accuracy: 1.000000\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss: 0.000021  validate_accuracy: 0.636200  train_accuracy: 1.000000\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss: 0.000038  validate_accuracy: 0.627400  train_accuracy: 1.000000\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss: 0.000043  validate_accuracy: 0.625000  train_accuracy: 1.000000\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss: 0.000052  validate_accuracy: 0.636800  train_accuracy: 1.000000\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss: 0.000020  validate_accuracy: 0.637000  train_accuracy: 1.000000\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss: 0.000034  validate_accuracy: 0.640000  train_accuracy: 1.000000\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss: 0.000019  validate_accuracy: 0.632000  train_accuracy: 1.000000\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss: 0.000018  validate_accuracy: 0.635800  train_accuracy: 1.000000\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss: 0.000036  validate_accuracy: 0.631400  train_accuracy: 1.000000\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss: 0.000013  validate_accuracy: 0.635600  train_accuracy: 1.000000\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss: 0.000023  validate_accuracy: 0.638000  train_accuracy: 1.000000\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss: 0.000025  validate_accuracy: 0.638800  train_accuracy: 1.000000\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss: 0.000045  validate_accuracy: 0.616600  train_accuracy: 1.000000\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss: 0.000109  validate_accuracy: 0.631600  train_accuracy: 1.000000\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss: 0.000055  validate_accuracy: 0.632000  train_accuracy: 1.000000\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss: 0.000031  validate_accuracy: 0.638600  train_accuracy: 1.000000\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss: 0.000048  validate_accuracy: 0.633000  train_accuracy: 1.000000\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss: 0.000028  validate_accuracy: 0.628400  train_accuracy: 1.000000\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss: 0.000064  validate_accuracy: 0.627800  train_accuracy: 1.000000\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss: 0.000023  validate_accuracy: 0.631000  train_accuracy: 1.000000\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss: 0.000014  validate_accuracy: 0.641800  train_accuracy: 1.000000\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss: 0.000039  validate_accuracy: 0.625800  train_accuracy: 1.000000\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss: 0.000035  validate_accuracy: 0.629400  train_accuracy: 1.000000\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss: 0.000045  validate_accuracy: 0.634200  train_accuracy: 1.000000\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss: 0.000034  validate_accuracy: 0.629600  train_accuracy: 1.000000\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss: 0.000038  validate_accuracy: 0.634800  train_accuracy: 1.000000\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss: 0.000078  validate_accuracy: 0.622200  train_accuracy: 1.000000\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss: 0.000029  validate_accuracy: 0.627200  train_accuracy: 1.000000\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss: 0.000150  validate_accuracy: 0.629400  train_accuracy: 1.000000\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss: 0.000032  validate_accuracy: 0.631000  train_accuracy: 1.000000\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss: 0.000041  validate_accuracy: 0.631400  train_accuracy: 1.000000\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss: 0.000042  validate_accuracy: 0.629000  train_accuracy: 1.000000\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss: 0.000023  validate_accuracy: 0.633200  train_accuracy: 1.000000\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss: 0.000067  validate_accuracy: 0.630400  train_accuracy: 1.000000\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss: 0.000022  validate_accuracy: 0.631200  train_accuracy: 1.000000\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss: 0.000060  validate_accuracy: 0.630800  train_accuracy: 1.000000\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss: 0.000084  validate_accuracy: 0.618800  train_accuracy: 1.000000\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss: 0.000021  validate_accuracy: 0.631600  train_accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss: 0.000105  validate_accuracy: 0.636800  train_accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss: 0.000019  validate_accuracy: 0.627400  train_accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss: 0.000025  validate_accuracy: 0.632800  train_accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss: 0.000040  validate_accuracy: 0.630000  train_accuracy: 1.000000\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss: 0.000020  validate_accuracy: 0.636000  train_accuracy: 1.000000\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss: 0.000065  validate_accuracy: 0.634000  train_accuracy: 1.000000\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss: 0.000026  validate_accuracy: 0.626000  train_accuracy: 1.000000\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss: 0.000036  validate_accuracy: 0.632000  train_accuracy: 1.000000\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss: 0.000022  validate_accuracy: 0.628800  train_accuracy: 1.000000\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss: 0.000016  validate_accuracy: 0.630200  train_accuracy: 1.000000\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss: 0.000046  validate_accuracy: 0.639200  train_accuracy: 1.000000\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss: 0.000008  validate_accuracy: 0.634000  train_accuracy: 1.000000\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss: 0.000047  validate_accuracy: 0.631800  train_accuracy: 1.000000\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss: 0.000035  validate_accuracy: 0.624200  train_accuracy: 1.000000\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss: 0.000017  validate_accuracy: 0.625200  train_accuracy: 1.000000\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss: 0.000042  validate_accuracy: 0.640400  train_accuracy: 1.000000\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss: 0.000017  validate_accuracy: 0.626200  train_accuracy: 1.000000\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss: 0.000042  validate_accuracy: 0.627600  train_accuracy: 1.000000\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss: 0.000060  validate_accuracy: 0.625800  train_accuracy: 1.000000\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss: 0.000024  validate_accuracy: 0.634600  train_accuracy: 1.000000\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss: 0.000042  validate_accuracy: 0.632600  train_accuracy: 1.000000\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss: 0.000011  validate_accuracy: 0.624200  train_accuracy: 1.000000\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss: 0.000261  validate_accuracy: 0.627400  train_accuracy: 1.000000\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss: 0.000038  validate_accuracy: 0.626400  train_accuracy: 1.000000\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss: 0.000014  validate_accuracy: 0.630800  train_accuracy: 1.000000\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss: 0.000062  validate_accuracy: 0.636600  train_accuracy: 1.000000\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss: 0.000018  validate_accuracy: 0.628200  train_accuracy: 1.000000\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss: 0.000123  validate_accuracy: 0.632600  train_accuracy: 1.000000\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss: 0.000049  validate_accuracy: 0.626200  train_accuracy: 1.000000\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss: 0.000010  validate_accuracy: 0.625600  train_accuracy: 1.000000\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss: 0.000042  validate_accuracy: 0.631000  train_accuracy: 1.000000\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss: 0.000014  validate_accuracy: 0.631000  train_accuracy: 1.000000\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss: 0.000078  validate_accuracy: 0.626400  train_accuracy: 1.000000\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss: 0.000064  validate_accuracy: 0.618400  train_accuracy: 1.000000\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss: 0.000023  validate_accuracy: 0.629200  train_accuracy: 1.000000\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss: 0.000039  validate_accuracy: 0.636600  train_accuracy: 1.000000\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss: 0.000036  validate_accuracy: 0.629600  train_accuracy: 1.000000\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss: 0.000025  validate_accuracy: 0.632200  train_accuracy: 1.000000\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss: 0.000030  validate_accuracy: 0.631200  train_accuracy: 1.000000\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss: 0.000020  validate_accuracy: 0.632400  train_accuracy: 1.000000\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss: 0.000125  validate_accuracy: 0.630200  train_accuracy: 1.000000\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss: 0.000012  validate_accuracy: 0.634200  train_accuracy: 1.000000\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss: 0.000027  validate_accuracy: 0.625200  train_accuracy: 1.000000\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss: 0.000046  validate_accuracy: 0.616400  train_accuracy: 1.000000\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss: 0.000052  validate_accuracy: 0.615800  train_accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss: 0.000064  validate_accuracy: 0.624000  train_accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss: 0.000021  validate_accuracy: 0.632800  train_accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss: 0.000078  validate_accuracy: 0.631000  train_accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss: 0.000068  validate_accuracy: 0.622200  train_accuracy: 1.000000\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss: 0.000023  validate_accuracy: 0.622000  train_accuracy: 1.000000\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss: 0.000149  validate_accuracy: 0.615400  train_accuracy: 1.000000\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss: 0.000049  validate_accuracy: 0.623800  train_accuracy: 1.000000\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss: 0.000021  validate_accuracy: 0.630000  train_accuracy: 1.000000\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss: 0.000039  validate_accuracy: 0.625000  train_accuracy: 1.000000\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss: 0.000050  validate_accuracy: 0.625200  train_accuracy: 1.000000\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss: 0.000068  validate_accuracy: 0.617600  train_accuracy: 1.000000\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss: 0.000138  validate_accuracy: 0.627000  train_accuracy: 1.000000\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss: 0.000019  validate_accuracy: 0.632800  train_accuracy: 1.000000\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss: 0.000082  validate_accuracy: 0.617000  train_accuracy: 1.000000\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss: 0.000069  validate_accuracy: 0.628400  train_accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss: 0.000164  validate_accuracy: 0.616200  train_accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss: 0.000024  validate_accuracy: 0.625800  train_accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss: 0.000021  validate_accuracy: 0.623800  train_accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss: 0.000020  validate_accuracy: 0.634600  train_accuracy: 1.000000\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss: 0.000026  validate_accuracy: 0.630400  train_accuracy: 1.000000\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss: 0.000038  validate_accuracy: 0.622200  train_accuracy: 1.000000\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss: 0.000011  validate_accuracy: 0.625000  train_accuracy: 1.000000\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss: 0.000026  validate_accuracy: 0.625800  train_accuracy: 1.000000\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss: 0.000069  validate_accuracy: 0.629200  train_accuracy: 1.000000\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss: 0.000042  validate_accuracy: 0.634600  train_accuracy: 1.000000\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss: 0.000119  validate_accuracy: 0.628200  train_accuracy: 1.000000\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss: 0.000011  validate_accuracy: 0.632000  train_accuracy: 1.000000\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss: 0.000012  validate_accuracy: 0.622600  train_accuracy: 1.000000\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss: 0.000017  validate_accuracy: 0.625600  train_accuracy: 1.000000\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss: 0.000025  validate_accuracy: 0.634400  train_accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss: 0.000021  validate_accuracy: 0.633200  train_accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss: 0.000005  validate_accuracy: 0.634600  train_accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss: 0.000014  validate_accuracy: 0.637600  train_accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss: 0.000024  validate_accuracy: 0.633600  train_accuracy: 1.000000\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss: 0.000014  validate_accuracy: 0.635400  train_accuracy: 1.000000\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss: 0.000028  validate_accuracy: 0.639200  train_accuracy: 1.000000\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss: 0.000013  validate_accuracy: 0.632800  train_accuracy: 1.000000\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss: 0.000022  validate_accuracy: 0.638400  train_accuracy: 1.000000\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss: 0.000011  validate_accuracy: 0.621200  train_accuracy: 1.000000\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss: 0.000012  validate_accuracy: 0.628800  train_accuracy: 1.000000\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss: 0.000037  validate_accuracy: 0.638400  train_accuracy: 1.000000\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss: 0.000006  validate_accuracy: 0.636800  train_accuracy: 1.000000\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss: 0.000010  validate_accuracy: 0.640800  train_accuracy: 1.000000\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss: 0.000013  validate_accuracy: 0.631000  train_accuracy: 1.000000\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss: 0.000055  validate_accuracy: 0.627600  train_accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss: 0.000030  validate_accuracy: 0.622200  train_accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss: 0.000008  validate_accuracy: 0.636000  train_accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss: 0.000013  validate_accuracy: 0.630400  train_accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss: 0.000008  validate_accuracy: 0.626800  train_accuracy: 1.000000\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss: 0.000017  validate_accuracy: 0.629000  train_accuracy: 1.000000\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss: 0.000039  validate_accuracy: 0.622800  train_accuracy: 1.000000\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss: 0.000017  validate_accuracy: 0.633400  train_accuracy: 1.000000\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss: 0.000008  validate_accuracy: 0.629200  train_accuracy: 1.000000\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss: 0.000015  validate_accuracy: 0.628000  train_accuracy: 1.000000\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss: 0.000022  validate_accuracy: 0.625200  train_accuracy: 1.000000\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss: 0.000036  validate_accuracy: 0.633600  train_accuracy: 1.000000\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss: 0.000008  validate_accuracy: 0.639400  train_accuracy: 1.000000\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss: 0.000005  validate_accuracy: 0.645200  train_accuracy: 1.000000\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss: 0.000011  validate_accuracy: 0.635000  train_accuracy: 1.000000\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss: 0.000016  validate_accuracy: 0.629200  train_accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss: 0.000036  validate_accuracy: 0.637400  train_accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss: 0.000005  validate_accuracy: 0.632200  train_accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss: 0.000011  validate_accuracy: 0.635000  train_accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss: 0.000015  validate_accuracy: 0.627600  train_accuracy: 1.000000\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss: 0.000015  validate_accuracy: 0.637400  train_accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss: 0.000010  validate_accuracy: 0.644000  train_accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss: 0.000007  validate_accuracy: 0.640200  train_accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss: 0.000010  validate_accuracy: 0.639400  train_accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss: 0.000007  validate_accuracy: 0.635400  train_accuracy: 1.000000\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss: 0.000013  validate_accuracy: 0.638600  train_accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss: 0.000012  validate_accuracy: 0.637200  train_accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss: 0.000005  validate_accuracy: 0.640800  train_accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss: 0.000008  validate_accuracy: 0.635000  train_accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss: 0.000044  validate_accuracy: 0.618400  train_accuracy: 1.000000\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss: 0.000011  validate_accuracy: 0.633400  train_accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss: 0.000036  validate_accuracy: 0.640400  train_accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss: 0.000003  validate_accuracy: 0.635800  train_accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss: 0.000011  validate_accuracy: 0.636400  train_accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss: 0.000014  validate_accuracy: 0.632400  train_accuracy: 1.000000\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss: 0.000009  validate_accuracy: 0.632800  train_accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss: 0.000276  validate_accuracy: 0.632000  train_accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss: 0.000018  validate_accuracy: 0.634800  train_accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss: 0.000009  validate_accuracy: 0.636400  train_accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss: 0.000017  validate_accuracy: 0.636800  train_accuracy: 1.000000\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss: 0.000029  validate_accuracy: 0.629200  train_accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss: 0.000126  validate_accuracy: 0.640400  train_accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss: 0.000004  validate_accuracy: 0.628400  train_accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss: 0.000008  validate_accuracy: 0.639200  train_accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss: 0.000024  validate_accuracy: 0.628000  train_accuracy: 1.000000\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss: 0.000008  validate_accuracy: 0.629400  train_accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss: 0.000031  validate_accuracy: 0.636600  train_accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss: 0.000002  validate_accuracy: 0.630400  train_accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss: 0.000018  validate_accuracy: 0.626800  train_accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss: 0.000446  validate_accuracy: 0.618800  train_accuracy: 1.000000\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss: 0.000084  validate_accuracy: 0.620800  train_accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss: 0.000034  validate_accuracy: 0.630000  train_accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss: 0.000023  validate_accuracy: 0.616400  train_accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss: 0.000842  validate_accuracy: 0.603600  train_accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss: 0.000071  validate_accuracy: 0.619600  train_accuracy: 1.000000\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss: 0.000078  validate_accuracy: 0.618000  train_accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss: 0.000042  validate_accuracy: 0.625000  train_accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss: 0.000017  validate_accuracy: 0.635800  train_accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss: 0.000056  validate_accuracy: 0.629000  train_accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss: 0.000142  validate_accuracy: 0.623200  train_accuracy: 1.000000\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss: 0.000014  validate_accuracy: 0.630200  train_accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss: 0.000020  validate_accuracy: 0.634600  train_accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss: 0.000032  validate_accuracy: 0.634800  train_accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss: 0.000015  validate_accuracy: 0.633000  train_accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss: 0.000042  validate_accuracy: 0.623400  train_accuracy: 1.000000\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss: 0.000009  validate_accuracy: 0.629000  train_accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss: 0.000058  validate_accuracy: 0.637800  train_accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss: 0.000039  validate_accuracy: 0.623400  train_accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss: 0.000026  validate_accuracy: 0.628200  train_accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss: 0.000212  validate_accuracy: 0.635400  train_accuracy: 1.000000\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss: 0.000012  validate_accuracy: 0.630200  train_accuracy: 1.000000\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss: 0.000033  validate_accuracy: 0.647600  train_accuracy: 1.000000\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss: 0.000044  validate_accuracy: 0.636200  train_accuracy: 1.000000\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss: 0.000015  validate_accuracy: 0.648600  train_accuracy: 1.000000\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss: 0.000015  validate_accuracy: 0.632600  train_accuracy: 1.000000\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss: 0.000007  validate_accuracy: 0.635400  train_accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss: 0.000021  validate_accuracy: 0.642400  train_accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss: 0.000011  validate_accuracy: 0.639400  train_accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss: 0.000014  validate_accuracy: 0.636600  train_accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss: 0.000018  validate_accuracy: 0.628200  train_accuracy: 1.000000\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss: 0.000010  validate_accuracy: 0.625000  train_accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss: 0.000059  validate_accuracy: 0.628400  train_accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss: 0.000022  validate_accuracy: 0.627200  train_accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss: 0.000013  validate_accuracy: 0.635600  train_accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss: 0.000014  validate_accuracy: 0.630800  train_accuracy: 1.000000\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss: 0.000007  validate_accuracy: 0.634600  train_accuracy: 1.000000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.623227171599865\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsvv62n4XAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3152430710>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
